<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>PK's Online Adventure || Blog</title>
    <!-- font-awesome -->
    <link
      href="./fontawesome-free-6.5.1-web/css/all.min.css"
      rel="stylesheet"
    />
    <!-- custom css -->
    <link rel="stylesheet" href="./css/styles.css" />
  </head>
  <body>
    <!-- Navbar -->
    <!-- nav button -->
    <span class="nav-btn" id="nav-btn">
      <i class="fas fa-bars"></i>
    </span>
    <!-- end nav button -->
    <nav class="navbar" id="navbar">
      <div class="navbar-header">
        <span class="nav-close" id="nav-close">
          <i class="fas fa-times"></i>
        </span>
      </div>
      <ul class="nav-items">
        <li><a href="index.html" class="nav-link">home</a></li>
        <li><a href="resume.html" class="nav-link">resume</a></li>
        <li><a href="architecture.html" class="nav-link">projects</a></li>
        <li><a href="blog.html" class="nav-link">blog</a></li>
      </ul>
    </nav>
    <!-- End of Navbar -->
    <!-- Header -->
    <header class="header-blog">
      <div class="banner">
        <h2>Welcome to</h2>
        <h1>PK's Online Adventure</h1>
      </div>
    </header>
    <!-- End of Header -->
    <!-- Content Divider -->
    <div class="content-divider"></div>
    <!-- End of Content Divider -->
    <section class="blog">
      <div class="section-center clearfix">
        <div class="section-title">
          <h2>The Journey</h2>
        </div>
        <!-- single card -->
        <article class="blog-card">
          <!-- blog info -->
          <div class="blog-info">
            <h3>Background - and why the Cloud Resume Challenge?</h3>
            <h4>10 November 2023</h4>
            <p>
              Prior to 2017 I had spent 19 years working for CSC (now DXC) in
              various solution design/solution architecture roles. My area of
              expertise was in large scale outsourcing of commodity IT services,
              which morphed into private/public/hybrid IaaS services. I was keen
              to try something else by the time the merger with HPE was
              announced, and so gratefully accepted a package in 2017.
              <br />
              <br />
              I made the decision to get back into the IT game around June 2023.
              Goes without saying that six years is a long time in IT, and the
              last six years especially so!
              <br />
              <br />
              My first activity was to load up on a courses and certifications,
              to bring myself up to speed with what's available and what's
              possible today. The ISC^2 courses were broad in nature and
              provided a good overview of the state of play, at least from a
              risk, regulatory and governance perspective. I then crammed my
              head full of over 200 acronyms and service descriptions to gain
              the AWS Practitioner cert.
              <br />
              <br />
              As I was clicking my way through the Azure online training I
              didn't feel I was getting a proper grip on it. For sure, I was
              confident I could sit in a room and make suggestions about how
              things should work, but it didn't feel like a solid foundation.
              <br />
              <br />
              I also realised that if I was to bridge a 6 year gap in IT work
              experience I would need to something to demonstrate all those
              "secondary" skills that made the best of my previous co-workers
              and direct reports stand out from the crowd - just in time
              learning, problem solving, initiative, resilience and persistence.
              <br />
              <br />
              Enter the
              <a href="https://cloudresumechallenge.dev/" target="_blank"
                >Cloud Resume Challenge!</a
              >
              To quote Forrest directly:
              <br />
              <br />
              "...the Cloud Resume Challenge isn't a tutorial or a how-to guide.
              It's a project spec, and not a very helpful one at that. It tells
              you what the outcome of the project should be, and provides enough
              structure that you don't go too far off the rails, but other than
              that - you're on your own!"
              <br />
              <br />
              In essence the challenge requires us to build a simple website
              using as many cloud-related services as possible. I do believe
              that getting hands-on with tech is the best way to learn and come
              up to speed with whatâ€™s possible. So, lets get cracking!
              <br />
              <br />
            </p>
          </div>
        </article>
        <!-- end of single card -->
        <!-- single card -->
        <article class="blog-card">
          <!-- blog info -->
          <div class="blog-info">
            <h3>Project Phase 1 - HTML, Azure Storage and CDN</h3>
            <h4>11 November 2023</h4>
            <p>
              HTML/CSS - the minimum requirement is to convert your resume to
              HTML/CSS. I've probably gone a bit further with the site than the
              challenge requires, but I haven't played with HTML/CSS before and
              found it pretty interesting.
              <br /><br />
              I came across John Smilga's HTML/CSS course on
              <a
                href="https://www.udemy.com/share/101A483@n8hnTgvNlCIZEY6RlytAjmNgbx692bUQsj5yKeORHPDwRh1Y8QIrpC0l-iBgW0na/"
                target="_blank"
                >Udemy.</a
              >
              <br />
              <br />
              I've still got one outstanding issue regarding responsiveness of
              image elements in a mobile browser, otherwise it all seems to
              work. So if you want to experience scrolling text over a fixed
              background, view the site on your laptop not your phone ðŸ™‚
              <br /><br />
              Domain Registration - registering my domain name with Cloudflare
              was pretty straight forward and cheap. By default they offer proxy
              services and general SSL certificate services, both of which I
              disabled. Microsoft CDN takes care of SSL.
              <br />
              <br />
              Azure Storage and CDN - to be honest nothing much to report here -
              it all just works as you would expect. Plenty of how-to's on the
              website.
              <br />
              <br />
              The Azure Tools in Visual Studio Code worked really well - editing
              code directly on the files stored in Azure. However once CDN was
              incorporated versioning issues between the source file and CDN's
              cache arose. So local editing and much purging of cache! This
              problem will be properly addressed once I complete the IaC and
              devOps phases.
              <br />
              <br />
              All up I've spent about 30 hours on the HTML side (course,
              experimenting and troubleshooting), but it only took an extra 3-4
              hours to get through the Azure and Cloudflare stuff.
              <br />
              <br />
              Up next..... Python and Azure Functions!
            </p>
          </div>
        </article>
        <!-- end of single card -->
        <!-- single card -->
        <article class="blog-card">
          <!-- blog info -->
          <div class="blog-info">
            <h3>
              Project Phase 2 - Azure Functions, Python, CosmosDB, Key Vault,
              Github
            </h3>
            <h4>28 January 2024</h4>
            <p>
              This section has taken me a bit longer, as I got bogged down with
              Javascript and Python courses (having never written a line of code
              in either prior to December last year), plus Christmas and summer
              holidays.
            </p>
            <br />
            <h5>Azure Functions with Python</h5>
            <p>
              The Microsoft doco refers to a new class-based programming model
              for Python introduced with Azure Functions Python v4+. The new
              approach removes the need for a separate "function.json" file
              containing bindings etc. I found this a bit of a problem given
              many Challengers have used the old version.
              <br />
              <br />
              Following on from the HTML piece I did all the coding in Visual
              Studio Code with a variety of extensions (Python, Github Pull and
              Request Issues, Azure Account, Azure Database, Azure Functions,
              Azure Resources, Azure Static Web Apps, Azure Storage and Azurite
              local storage emulator). Along with the obvious benefits I found
              these helped with a lot of behind-the-scenes config while
              deploying a function from local into Azure, converting local json
              settings to Application Settings for example.
              <br />
              <br />
              The MS articles were a good starting point for creating a function
              locally, then deploying to Azure. The quickstart files also cover
              connecting to a CosmosDB instance.
              <br />
              <br />
              <a
                href="https://learn.microsoft.com/en-us/azure/azure-functions/functions-add-output-binding-cosmos-db-vs-code?pivots=programming-language-python&tabs=isolated-process%2Cv1"
                target="_blank"
                >Connect Azure Functions to Azure Cosmos DB using Visual Studio
                Code.</a
              >
              <br />
              <a
                href="https://learn.microsoft.com/en-us/azure/azure-functions/functions-develop-vs-code?tabs=node-v3%2Cpython-v2%2Cisolated-process&pivots=programming-language-python"
                target="_blank"
                >Develop Azure Functions by using Visual Studio Code.</a
              >
              <br />
              <br />
              Regarding CosmosDB, I want to acknowledge a fellow challenger
              techbrett for the query solution, as its a much more databasey way
              of doing things than the python code I was struggling with:
              <br />
              <a
                href="https://github.com/techbrett/azure-resume/"
                target="_blank"
                >techbrett</a
              >
              <br />
              <br />
              I've implemented Azure Key Vault to store any credentials or
              access keys required by the solution. The variables defined in
              Azure Application Settings now contain a pointer to the key vault
              rather than the access key itself. There are VSC extensions
              allowing you to read from the Key Vault when doing your local
              development but I haven't gone down that rabbit hole yet. This
              article was helpful, although the screens look slightly different:
              <br />
              <br />
              <a
                href="https://levelup.gitconnected.com/a-secure-way-to-use-credentials-and-secrets-in-azure-functions-7ec91813c807"
                target="_blank"
                >A Secure way to use Credentials and Secrets in Azure Functions
                | by Dhyanendra Singh Rathore</a
              >
              <br />
              <br />
            </p>
            <h5>Github integration and automated deployment</h5>
            <p>
              In hindsight, one should have set this up prior to writing any
              code ðŸ™‚ That said, adding it in afterwards wasn't too much of an
              issue, although strangely Github kept tracking changes on a couple
              of files that I didn't want synched with the repository, even
              though they were added to the .gitignore file. I had to use git
              bash to manually exclude them. ChatGPT to the rescue!
              <br />
              <br />
              I've created two deployment workflows - one for front end and one
              for backend. Make sure when creating the service account as per
              the MS doco you make it a contributor for both the storage RG and
              CDN RG (if you're using CDN). They work well, although both are
              triggered any time a change is made to the main branch, and using
              the commands outlined in the MS doco, that means all files are
              copied up every time. While I'm still making regular changes to
              the website I've reset the workflow to manual for now. Also need
              to think about whether every file needs to be copied up or whether
              a differential copy is more efficient, in which case need to use
              azcopy synch instead.
              <br />
              <br />
              <a
                href="https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-static-site-github-actions?tabs=userlevel"
                target="_blank"
                >Use GitHub Actions to deploy a static site to Azure Storage</a
              >
              <br />
              <a
                href="https://learn.microsoft.com/en-us/azure/azure-functions/functions-how-to-github-actions?tabs=linux%2Cdotnet&pivots=method-manual"
                target="_blank"
                >Use GitHub Actions to make code updates in Azure Functions</a
              >
              <br />
              <br />
            </p>
            <h5>Conclusion and next steps</h5>
            <br />
            <br />
            <p>
              This part of the challenge made my head hurt ðŸ™‚ There are a number
              of moving parts and new concepts all being deployed in parallel
              while being discovered for the first time - IDE integration with
              Github and Azure, Python, local versus remote deployments etc etc.
              Of all the components, practices and processes related to this
              solution, I'd say its my troubleshooting skills have been improved
              the most, and ChatGPT has been indispensible.
              <br />
              <br />
              The two key elements of the challenge that I haven't completed are
              testing and IaC. I'm leaving them for now, as I need to cycle back
              and finish the Python and Javascript courses. Also, I'm heading
              down data path rather than a devops engineer/architect path, and
              have a couple of projects that I want to get cracking on. I
              believe a big tick for me will be the ability to deploy a
              cloud-based platform upon which the data analysis can be run. This
              project has therefore given me a strong foundation for the next
              phase.
            </p>
          </div>
        </article>
        <!-- end of single card -->
        <!-- single card -->
        <article class="blog-card">
          <!-- blog info -->
          <div class="blog-info">
            <h3>New Projects and ChatGPT/GPT Plus</h3>
            <h4>14 February 2024</h4>
              <h5>The Weather Forecast Project</h5>
              <p>
              With the Resume project more or less complete, I started to look for ideas as to how I could utilise the Azure platform, and expand my very primitive coding skills. One of the courses I was taking demonstrated how to gather weather data from the OpenWeatherMaps API. My thinking was to take that raw data, turn it into a prompt and have OpenAI's API generate a forecast based on that data. This would give me experience gathering data from multiple sources programmatically, incorporate some data transformation activities, and of course make use of OpenAI's API. Building on the resume project, I would then convert this code to an Azure Function, and make use of the Key Vault to keep my precious API keys out of my code.
              <br>
              <br>
                <UL> Some of the issues I faced, and learnings taken include:
                  <li>LOGGING! - a Udemy course by <a href="https://www.udemy.com/user/steveavon/" target = "_blank">Steve Avon</a> included a comprehensive model to implement which was a revelation. Since Azure integrates with the Python logging module all my logging could be retained from local deployment through to production deployment as an Azure Function. I almost feel like a real app guy....</li>
                  <li>Variables - Populating variables based on user input, passing variables from frontend to backend, losing track of what I called them, where I put them....</li>
                  <li>Data types and structures - Extracting and processing a raw dump of data collected via the OpenWeatherMap API, dealing with lists, dictionaries, lists of dictionaries, and landing on a text structure that that OpenAI would accept. This was resolved through a lot of trial and error, iterating with my GPTPlus subscription until "we" got it right.</li>
                  <li>Format of response from OpenAI - the API's JSON response format was unhelpful, so more Python code required to convert its standard response format (Python string) into a JSON file for presentation through the webpage. </li>
                  <li>Additional Key Vault learnings - the CosmosDB service natively interacts with the Azure Key Vault, unfortunately 3rd party APIs don't. So, additional code was required to pull the API keys from the Vault and package them up with the request to OpenWeatherMaps and OpenAI.</li>
                </UL>
              </p>
              <br>
              <br>
              <h5>ChatGPT and GPTPlus</h5>
              <p>
              I should mention that I have recently picked up a new best (non-human) friend - a GPTPlus subscription. The pros and cons of LLMs and AI assistants has been flogged to death elsewhere. What I can share is the impact its had on my attitude and thinking.
              <br>
              <br>
              Its fair to say that towards the end of my time with CSC I had fallen into the trap of saying "NO, and this is why" to any left field or new idea. Maybe a bit of outsourcing PTSD..... Anyway, being completely honest, that thinking was still lurking in the shadows of my brain when I commenced this journey. After playing with GPTPlus every day now for two months I find myself saying "why not" to almost any crazy idea. By learning to ask the right questions, iterating through problems with the GPT, I'm finding that the main limitation at this stage in my learning is my own imagination and awareness of what's actually possible. That is a fundamental shift in my perception.
              <br>
              <br>
              For example, my next project will involve correlating a decade worth of posts from various stock market subreddits with the share price of the relevant stock or commodity, to ascertian whether there is any predictive value in keeping tabs on those shouty Reddit and Facebook groups over the short/medium/long term. Without GPTPlus, I'd be viewing a project like this as aspirational - hopefully after a couple of years I'd have the syntax burned into my brain to make such a task a reality. And yet right now I've already sourced the data from Reddit and am in the process of manipulating and cleansing the data.
              <br>
              <br>
              I do not wish to represent myself as something I'm not at this point in the journey. I haven't suddenly gained a decade of coding experience. But I can explain every line of code committed to Github. At the end of the day, the onus is on the individual - on me - to make sure I am always learning from the GPT, not just leaning on it. I do this by ensuring GPTPlus comments all code, and explains every line to me. I also include various phrases in the prompt such as 'best practice', 'most secure', and I also ask for alternative approaches. It does get it wrong sometimes - its still pushing completion methods that OpenAI themselves have actually deprecated! As long as I remember this, and don't do a Michael Cohen or a Channel 9, at a minimum it will fast track my education.
              <br>
              <br>
              Stay tuned for the results of my stockreddit data analysis extravaganza!
              </p>
          </div>
        </article>
        <!-- end of single card -->
        <!-- single card -->
        <article class="blog-card">
          <!-- blog info -->
          <div class="blog-info">
            <h3>New Projects - Data Analytics</h3>
            <h4>27 February 2024</h4>
              <h5>Data Project 1 - Reddit, COVID and Gamestop</h5>
              <p>
              My initial goal for this project was to determine whether there was any value in trawling through stock-related social media posts, in order to identify or even predict large moves up or down of the mentioned stocks. While this is still the end goal, another story emerged as I started to look at the high-level data, that I thought was worth documenting. So consider this as part 1 of 2.
              <br>
              <br>
              The approach was to plot the number of comments or posts on a social media platform against actual market prices, to see whether there was any correlation or predictive quality between retail investor sentiment expressed through social media, and actual stock or market prices. I would start at the highest level (total posts from a popular social media platform plotted against the S&P500), and then drill down into specific stocks. To continue the theme of exposing myself to the latest Cloud services, I would present this data through Tableau Public.
              <br>
              <br>
              My first task was to gather as much historical data from social media sources as I could, as cheaply as possible. This ruled out using X, as Elon wants to charge $100 a month for access to his API to help send him to Mars or something. Reddit make their API available to all for free, but with limitations as to how much data can be accessed. Fortunately, a Reddit group called r/pushshift had been archiving ALL (yes, ALL, even the dodgy ones....) subreddits since 2008 and storing them on their site <a href="https://academictorrents.com/" target = "_blank">Academic Torrents.</a> As these archives are quite large, one member <a href="https://github.com/Watchful1/PushshiftDumps" target = "_blank">(Watchful1)</a>> has also created a bunch of Python scripts to manipulate and extract data without needing a large server to do so (big thanks!). After some mods to these scripts I ended up with a csv file with the count of all comments from 20 or so stock-related subreddits dating back to 2008.
              <br>
              <br>
                <ul>When this data was plotted over time, three distinct phases were observed:
                  <li>2008 - 2019: Steady growth in activity, roughly doubling every year</li>
                  <li>Feb-Mar 2020: A 900% increase in activity in two months, with this level sustained over the remaining year</li>
                  <li>Jan 2021: A huge spike in activity, with over 9M comments in one week</li>
                </ul>
              </p>
              <br>
              <p>
              The next step was to overlay this data with the price data from the S&P500, to see if these phases were in any way related to market action. Through my trading activity I already had historical prices for stocks and stock market indexes going back several decades.
              <br>
              <br>
              The first period you would describe as uneventful - public interest steadily returning to the stock market after the pain of the GFC.
              <br>
              <br>
              The second phase coincided exactly with one of the most traumatic periods in recent history - the COVID pandemic, and specifically, the stockmarket crash that it precipitated. The S&P500 peaked February 2nd, and then lost 36% of its value before bottoming March 23rd. Comments in the stock related subreddits exploded from 300k total for   he month of December 2019, to 2.8M for the month of March 2020, remaining above 1.3M per month for the rest of the year. By this stage, r/wallstreetbets had over 1.2M members.
              <br>
              <br>
              Fear, layoffs, furloughs, capital destruction, lockdowns and time on our hands proved the perfect honeypot for people wanting to protect their wealth, find alternative sources of income, and probably most importantly, feel connected to something. Add to that a burgeoning Robin Hood investor mentality, a desire to "stick it to the man", and we now have a very large number of emotive speculators in one spot needing an opportunity to express themselves.
              <br>
              <br>
              However, looking at the S&P500 chart, there is nothing of note in the price action that would suggest an almost tenfold increase in activity going into January 2021. No spike, no crash, just normal market action. So what gives? A quick Google search for anything interesting happening in the stock market on the date of 21 Jan 2021 revealed the answer (and also illustrated how I personally had buried all memory of that period!!!).
              <br>
              <br>
              Back in 2021 a bricks-and-mortar video game store called Gamestop (NYSE:GME) made the news by gaining 2,400% in just over a month. The driver behind this price move was - surprise surprise - a Reddit community called r/wallstreetbets - a collection of individual online retail investors that somehow banded together to force the share price to the moon in spite of the best efforts of the larger hedge funds. I won't go into the back story here - its reasonably well covered in the Netfilx docuseries <a href="https://www.imdb.com/title/tt14036920/" target = "_blank">Eat The Rich: The Gamestop Saga.</a>
              <br>
              <br>
              So... while searching for an edge in the stock market, I (completely inadvertantly!) ended up showing not only the effect of the r/wallstreetbets phenomenon, but also its most significant cause - the COVID pandemic gathering a large number of emotively charged humans together in a short period of time. Who says data is dull? ðŸ™‚
              <br>
              <br>
                <UL> Some of the issues I faced, and learnings taken include:
                  <li>Large data sets should not be processed on a laptop. There is a reason why servers exist! As stated above, a big thanks to watchful1 for those Python scripts - I was able to extract what I needed without having to load up the entire archive into Jupyter Notebooks or Excel. I did try, and the results were....humourous!</li>
                  <li>As this is my first exposure to Tableau, its pretty basic with respect to data anaylsys. In reality I've only got three fields that I'm plotting here, so I'm not making much use of its BI capabilities. I also had some lovely annotations linking the different elements that helped the narrative a lot, thanks to an extension that.... disappeared when I tried to save up to Tableau Public. The developers do say it works best with the Server installation of Tableau, so I guess you get what you pay for ðŸ™‚</li>
                </UL>
              </p>
              <br>
              <br>
          </div>
        </article>
        <!-- end of single card -->
      </div>
    </section>

    <!-- footer -->
    <footer class="footer">
      <div class="section-center">
        <h4 class="footer-text">
          &copy;<span>2023</span>
          <span>PK's Online Adventure</span>
          all rights reserved
        </h4>
      </div>
    </footer>
    <!-- end of footer -->
    <script src="app.js"></script>
  </body>
</html>
